{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting\n",
    "\n",
    "For the first point, We started downloading the yellow cab data for January 2018(yellow_tripdata_2018-01.csv). Our plan was to make our studies and analysis firstly for one month and then extend our results considering also other years.\n",
    "Our Data-set contains a really large amount of data so we opted to choose the simple pd.read_csv() module from pandas.\n",
    "After considering many options such as Dask DataFrames, we decided to use the pandas importing method together with the chucksize option. The solution to working with a massive file with eight milions of lines is to load the file in smaller chunks and analyze with the smaller chunks. Here is our code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime tpep_dropoff_datetime  trip_distance  PULocationID  \\\n",
      "0  2018-01-01 00:21:05   2018-01-01 00:24:23            0.5            41   \n",
      "1  2018-01-01 00:44:55   2018-01-01 01:03:05            2.7           239   \n",
      "2  2018-01-01 00:08:26   2018-01-01 00:14:21            0.8           262   \n",
      "3  2018-01-01 00:20:22   2018-01-01 00:52:51           10.2           140   \n",
      "4  2018-01-01 00:09:18   2018-01-01 00:27:06            2.5           246   \n",
      "\n",
      "   total_amount  \n",
      "0          5.80  \n",
      "1         15.30  \n",
      "2          8.30  \n",
      "3         34.80  \n",
      "4         16.55  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def data_aggregator(path,columnnumber,chunksize):\n",
    "    df_list = []\n",
    "    for chunk in pd.read_csv(path,usecols=columnnumber, chunksize=chunksize):\n",
    "        df_list.append(pd.DataFrame(chunk))\n",
    "    result = pd.concat(df_list)\n",
    "    del df_list\n",
    "    result1=result.fillna(value=0)\n",
    "    del result\n",
    "    return result1\n",
    "JanData=\"yellow_tripdata_2018-01.csv\" #We define the path of our data(of January)\n",
    "JanDF=data_aggregator(JanData,[1,2,4,7,16],10000)\n",
    "print(JanDF.head())\n",
    "#With this function we can easily import data we need. We select the columns and the dataset we desire and, as we said, \n",
    "#we use chunksize the make the function run faster(and to avoid memory problems). We have printed the head of the dataframe to\n",
    "#show that everything was correct and the df contained columns corresponding to [1,2,4,7,16] indexs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the function to import the data correctly, We were asked to merge our data with a .json file in the homework repo  ([.json](https://github.com/CriMenghini/ADM-2018/blob/master/Homework_2/taxi_zone_lookup.csv)). The data was embedd in the page source-code so we had to apply our knowledge in web-scraping ang get the lines of sourcecode we needed. We used the **Beautiful** package. We firstly collected all the location ids, then the boroughs, then the zones and finally the serving zones. We have done that noticing that in the sourcecode all the above information required where all starting with a <td> and we simply went trough all the values starting with it. After that we merged all together in a Pandas DataFrame called boroghFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page = requests.get(\"https://github.com/CriMenghini/ADM-2018/blob/master/Homework_2/taxi_zone_lookup.csv\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "ids=[]\n",
    "bor=[]\n",
    "zon=[]\n",
    "srv_zon=[]\n",
    "cell=0\n",
    "for i in range(2,1327,5):    #Firstly We get all the location ids\n",
    "    a=soup.find_all('td')[i].get_text()  \n",
    "    ids.append(a)\n",
    "    cell=cell+1\n",
    "    \n",
    "cell=0\n",
    "for i in range(3,1328,5):    #Then all the boroughs\n",
    "    a=soup.find_all('td')[i].get_text()\n",
    "    bor.append(a)\n",
    "    cell=cell+1\n",
    "    \n",
    "cell=0\n",
    "for i in range(4,1329,5):   #After that we get al zones\n",
    "    a=soup.find_all('td')[i].get_text()\n",
    "    zon.append(a)\n",
    "    cell=cell+1\n",
    "cell=0\n",
    "for i in range(5,1330,5):    #Finally we get all the seving zones\n",
    "    a=soup.find_all('td')[i].get_text()\n",
    "    srv_zon.append(a)\n",
    "    cell=cell+1\n",
    "# after getting all the informations needed we merged all the lists together\n",
    "data_tuples = list(zip(ids,bor,zon,srv_zon))\n",
    "boroughFrame=pd.DataFrame(data_tuples,columns = [\"PULocationID\", \"Borogh\", \"Zone\", \"srv_zon\"]) #name columns assigned\n",
    "boroughFrame['PULocationID']=boroughFrame['PULocationID'].apply(int) #We wanted to be sure that PuLocationId were all ints.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we finally merged our Dataframes defining this function. We merged using the PULocation id column. **So we assumed that the LocationID in the .json file was equal to PULocationId in taxidataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8759874\n"
     ]
    }
   ],
   "source": [
    "def data_aggregator2(df1,df2,oncolumns,jointype):\n",
    "    result = pd.merge(df1, boroughFrame, on=oncolumns,how=jointype)\n",
    "    return result\n",
    "JanData=\"yellow_tripdata_2018-01.csv\" #We define the path of our data(of January)\n",
    "JanBoroughData=data_aggregator2(JanDF,boroughFrame,['PULocationID'],\"inner\")  \n",
    "print(len(JanBoroughData))\n",
    "del JanDF,boroughFrame,page,soup,ids,bor,zon,srv_zon \n",
    "#we also clean our memory deleting all the valuables we do not need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also noticed, exploring our dataset, that we had some values simply not reasonable and we decided to get rid of them. For example, for some rows the trip distance was equal to 0 or negative and that values would have impacted our analysis. We lost approximately 55000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8704498\n"
     ]
    }
   ],
   "source": [
    "JanBoroughData = JanBoroughData[JanBoroughData.trip_distance >0]\n",
    "print(len(JanBoroughData))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
